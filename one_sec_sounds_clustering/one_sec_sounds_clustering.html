<!DOCTYPE html>
<html>

<head>
  <meta charset=utf-8>
  <title>
    one_sec_sounds_clustering
  </title>

  <!-- highlight.js -->
  <link rel="stylesheet" href="../script/highlight/idea.css">
  <script src="../script/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y"
    crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx"
    crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"
    crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


  <style>
    body {
      max-width: 704px;
      margin: auto;
      padding: 32px 8px;
    }

    img {
      max-width: 100%;
    }

    video {
      max-width: 100%;
    }

    kbd {
      border-style: solid;
      border-width: 1px 2px 2px 1px;
      border-radius: 2px;
      padding: 2px;
      margin: 2px;
    }

    a {
      text-decoration: none;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      color: #004444;
      border-left: solid 8px #eeeeee;
      padding-left: 8px;
    }

    table {
      border-spacing: 0px;
      border-collapse: separate;
      border: 1px solid #dddddd;
    }

    tr.header {
      border: 1px solid black;
    }

    tr.odd {
      background: #eeeeee;
    }

    tr.even {
      background: #ffffff;
    }

    th {
      height: 2em;
      font-weight: normal;
      color: #004444;
      padding-left: 0.35em;
      padding-right: 0.35em;
      border-bottom: 3px solid #dddddd;
      border-left: 1px solid #dddddd;
    }

    td {
      height: 1.5em;
      padding-left: 0.35em;
      padding-right: 0.35em;
      border-bottom: 1px solid #dddddd;
      border-left: 1px solid #dddddd;
    }

    code {
      color: #004444;
    }

    pre code {
      border: 4px solid #eeeeee;
    }

    li {
      margin: 8px;
    }

    footer {
      border-top: 1px gray solid;
      padding: 0.5em;
      margin-top: 1em;
    }

    canvas {
      /* image-rendering: pixelated; */
      display: inline-block;
      border-style: solid;
      border-width: 1px;
      border-color: #627f84;
    }

    .controlBlock {
      display: inline-block;
      width: 450px;
      text-align: left;
      vertical-align: top;
      margin-left: 4px;
    }

    input[type="button"] {
      background-color: #ffffff;
      border: 2px solid #aaaaaa;
      font-size: 16px;
      height: 32px;
    }

    input[type="button"]:hover {
      background-color: #ffffff;
      border: 2px solid #aaccff;
    }

    div.numberInput {
      display: block;
      white-space: nowrap;
    }

    div.numberInput:hover {
      background-color: #e0ecff;
    }

    .numberInputLabel {
      /* max 12 letter  */
      display: inline-block;
      margin: 0 8px 0 8px;
      text-align: left;
      vertical-align: middle;
      width: 100px;

      font-size: 10pt;
      font-family: 'Courier New', Courier, monospace;
    }

    .numberInputNumber {
      display: inline-block;
      vertical-align: middle;
      width: 120px;
    }

    .numberInputRange {
      display: inline-block;
      vertical-align: middle;
      width: 160px;
    }

    .pullDownMenu {
      display: inline-block;
      text-align: center;
    }

    .pullDownMenu:hover {
      background-color: #e0ecff;
    }

    select {
      background-color: #ffffff;
      border: 2px solid #aaaaaa;
      height: 24px;
      vertical-align: middle;
      font-size: 12px;
    }

    select:hover {
      background-color: #ffffff;
      border: 2px solid #aaccff;
    }
  </style>
</head>

<body>
  <h1 id="秒以下の音のクラスタリング">1秒以下の音のクラスタリング</h1>
<p><a href="../golly_generations_clustering/golly_generations_clustering.html">GollyのGenerationsルールから生成した音のクラスタリング</a>が意外とうまく行ったので、1秒以下の音のクラスタリングを試しました。</p>
<h2 id="コードについて">コードについて</h2>
<p>Python3では次のライブラリを使っています。</p>
<ul>
<li><a href="https://matplotlib.org/">matplotlib</a></li>
<li><a href="https://pysoundfile.readthedocs.io/en/0.9.0/">PySoundFile</a></li>
<li><a href="https://python-speech-features.readthedocs.io/en/latest/#">python_speech_features</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">scikit-learn</a></li>
<li><a href="https://www.scipy.org/">SciPy, NumPy</a></li>
</ul>
<h2 id="データセット">データセット</h2>
<p>データセットに含まれる音は、長さを1秒以下、サンプリング周波数を44100Hzにそろえています。</p>
<p>今回作ったデータセットの大きさは6016サンプルで長さは約81分です。</p>
<h3 id="シンセサイザの音">シンセサイザの音</h3>
<p>以前作ったシンセサイザから、それぞれ1000ほどのサンプルをレンダリングしました。</p>
<p>今回使ったシンセサイザです。</p>
<ul>
<li><a href="https://ryukau.github.io/Singen0.2/">Singen0.2</a></li>
<li><a href="https://ryukau.github.io/Singen0.3/">Singen0.3</a></li>
<li><a href="https://ryukau.github.io/Pluck/">Pluck</a></li>
<li><a href="https://ryukau.github.io/KSCymbal/">KSCymbal</a></li>
<li><a href="https://ryukau.github.io/PADcymbal/">PADcymbal</a></li>
<li><a href="https://ryukau.github.io/FDNCymbal/">FDNCymbal</a></li>
<li><a href="https://ryukau.github.io/WaveCymbal/">WaveCymbal</a></li>
</ul>
<p>ブラウザのデベロッパツールのコンソールから次のコードを実行してレンダリングしました。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode javascript"><code class="sourceCode javascript"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">var</span> title <span class="op">=</span> <span class="va">document</span>.<span class="at">getElementsByTagName</span>(<span class="st">&quot;title&quot;</span>)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">var</span> button <span class="op">=</span> title[<span class="dv">0</span>].<span class="at">innerText</span> <span class="op">===</span> <span class="st">&quot;Singen0.3&quot;</span></a>
<a class="sourceLine" id="cb1-3" data-line-number="3">  <span class="op">?</span> <span class="va">ui</span>.<span class="at">buttonRandom</span> : buttonRandom</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="kw">var</span> counter <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="kw">var</span> id <span class="op">=</span> <span class="va">window</span>.<span class="at">setInterval</span>(</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">  () <span class="op">=&gt;</span> <span class="op">{</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    <span class="cf">if</span> (counter <span class="op">&gt;</span> <span class="dv">1024</span>) <span class="at">clearInterval</span>(id)</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">    <span class="va">button</span>.<span class="at">onClick</span>()</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    <span class="op">++</span>counter</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">  <span class="op">},</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11">  <span class="dv">1000</span></a>
<a class="sourceLine" id="cb1-12" data-line-number="12">)</a></code></pre></div>
<p>Pluck, KSCymbal, PADcymbal, FDNCymbal, WaveCymbalはステレオでレンダリングした音をチャンネルごとに分離してそれぞれ別のサンプルとして扱っています。</p>
<p>得られた音を次のコードで正規化しました。<a href="http://sox.sourceforge.net/">SoX</a>を使っています。</p>
<ul>
<li><a href="https://github.com/ryukau/filter_notes/blob/master/one_sec_sounds_clustering/demo/split.py">シンセサイザの音を加工するコードを読む (github.com)</a></li>
</ul>
<h3 id="フィールドレコーディングの音">フィールドレコーディングの音</h3>
<p>近所を歩いてフィールドレコーディングした音をデータセットに加えました。</p>
<p>フィールドレコーディングの音はSoXの <code>remix -</code> でモノラルにしてから1秒間隔で切っています。</p>
<ul>
<li><p><a href="https://github.com/ryukau/filter_notes/blob/master/one_sec_sounds_clustering/demo/slice.py">フィールドレコーディングの音を加工するコードを読む (github.com)</a></p></li>
<li><p><a href="http://sox.sourceforge.net/sox.html">SoX man page</a> - EFFECTSの節に <code>remix</code> の解説</p></li>
</ul>
<h2 id="特徴抽出">特徴抽出</h2>
<h3 id="mfccデルタ">MFCC+デルタ</h3>
<p><code>python_speech_features.mfcc</code> で取り出したMFCCを <code>numpy.ravel</code> で1次元にして <code>sklearn.cluster.AffinityPropagation</code> でクラスタリングしました。</p>
<p>今回使った <code>python_speech_features.mfcc</code> のパラメータです。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="im">import</span> python_speech_features</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="im">import</span> soundfile</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">data, samplerate <span class="op">=</span> soundfile.read(<span class="st">&quot;path/to/wav_file&quot;</span>)</a>
<a class="sourceLine" id="cb2-5" data-line-number="5"></a>
<a class="sourceLine" id="cb2-6" data-line-number="6">nfft <span class="op">=</span> <span class="dv">1024</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">mfcc <span class="op">=</span> python_speech_features.mfcc(</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">    data,</a>
<a class="sourceLine" id="cb2-9" data-line-number="9">    samplerate,</a>
<a class="sourceLine" id="cb2-10" data-line-number="10">    winlen<span class="op">=</span>nfft <span class="op">/</span> samplerate,</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">    winstep<span class="op">=</span><span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">    numcep<span class="op">=</span><span class="dv">26</span>,</a>
<a class="sourceLine" id="cb2-13" data-line-number="13">    nfilt<span class="op">=</span><span class="dv">52</span>,</a>
<a class="sourceLine" id="cb2-14" data-line-number="14">    nfft<span class="op">=</span>nfft,</a>
<a class="sourceLine" id="cb2-15" data-line-number="15">    lowfreq<span class="op">=</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2-16" data-line-number="16">    highfreq<span class="op">=</span><span class="dv">20000</span>,</a>
<a class="sourceLine" id="cb2-17" data-line-number="17">    preemph<span class="op">=</span><span class="fl">0.0</span>,</a>
<a class="sourceLine" id="cb2-18" data-line-number="18">    ceplifter<span class="op">=</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2-19" data-line-number="19">)</a></code></pre></div>
<p>今回は1秒以下の音のクラスタリングなので MFCC のフレーム数が <code>1 / winstep = 100</code> になるように調整しました。フレーム数が100より少ないときは0で埋めたフレームを付け足しています。フレーム数が100より大きいときは、音の始まりから100フレームだけを取り出して、残りのフレームを切り捨てています。</p>
<p><a href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">Practical Cryptography の MFCC チュートリアル</a>にMFCCのデルタで結果が改善することがあると書いてあったので <code>numpy.concatenate</code> でMFCCと、MFCCのデルタをつないで一つのデータポイントにまとめました。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># mfcc の取得は省略。</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">delta <span class="op">=</span> python_speech_features.base.delta(mfcc, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">mfccdelta <span class="op">=</span> numpy.concatenate((numpy.ravel(mfcc), numpy.ravel(delta)))</a></code></pre></div>
<p>ここでは <code>mfccdelta</code> のことをMFCC+デルタと呼んでいます。</p>
<h3 id="エンベロープ">エンベロープ</h3>
<p>ここでのエンベロープは信号を短い区間に区切って、それぞれの区間で信号の絶対値の最大値を取り出したものです。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">def</span> get_envelope(data, samplerate, winstep, n_frame):</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="co">    :data: 一次元の信号。</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="co">    :samplerate: サンプリング周波数。</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="co">    :winstep: 区間の長さ。秒。</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6"><span class="co">    :n_frame: 区間の数。</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">    data_abs <span class="op">=</span> numpy.<span class="bu">abs</span>(data)</a>
<a class="sourceLine" id="cb4-9" data-line-number="9">    envelope <span class="op">=</span> numpy.zeros(n_frame)</a>
<a class="sourceLine" id="cb4-10" data-line-number="10">    index <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">    step <span class="op">=</span> <span class="bu">int</span>(winstep <span class="op">*</span> samplerate)</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">    start <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13">    end <span class="op">=</span> step</a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    <span class="cf">for</span> frame <span class="kw">in</span> <span class="bu">range</span>(n_frame):</a>
<a class="sourceLine" id="cb4-15" data-line-number="15">        <span class="cf">if</span> end <span class="op">&gt;=</span> <span class="bu">len</span>(data_abs):</a>
<a class="sourceLine" id="cb4-16" data-line-number="16">            index <span class="op">=</span> frame</a>
<a class="sourceLine" id="cb4-17" data-line-number="17">            <span class="cf">break</span></a>
<a class="sourceLine" id="cb4-18" data-line-number="18">        envelope[frame] <span class="op">=</span> numpy.<span class="bu">max</span>(data_abs[start:end])</a>
<a class="sourceLine" id="cb4-19" data-line-number="19">        start <span class="op">=</span> end</a>
<a class="sourceLine" id="cb4-20" data-line-number="20">        end <span class="op">+=</span> step</a>
<a class="sourceLine" id="cb4-21" data-line-number="21">    <span class="cf">if</span> index <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb4-22" data-line-number="22">        envelope[index] <span class="op">=</span> numpy.<span class="bu">max</span>(data_abs[start:])</a>
<a class="sourceLine" id="cb4-23" data-line-number="23">        index <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb4-24" data-line-number="24">    <span class="cf">return</span> envelope</a></code></pre></div>
<p>エンベロープの区間の長さと数はMFCCの対応するパラメータと合わせました。区間の長さは <code>winstep=0.01</code> 、区間の数は <code>n_frame=100</code> です。</p>
<h3 id="ソートしたピッチ">ソートしたピッチ</h3>
<p>ピッチは <code>python_speech_features.sigproc.framesig</code> で区切った各フレームから次の手順で取り出しました。</p>
<ol type="1">
<li>NSD type II の局所最大点を全て取り出す。</li>
<li>局所最大点を降順にソート。</li>
<li>ソートした局所最大点の前から18個のインデックスを取り出して周波数を計算。</li>
<li>周波数をセント値に変換。</li>
<li>セント値を昇順にソート。</li>
</ol>
<p>手順 5. のソートがないと似たような音でまとまりにくくなります。推定したピッチは似たような値で順番が入れ替わっていることが多いので、ソートなしだとクラスタリングで計算される距離が大きくなることが予想されます。</p>
<p>CMND type II も試したのですが、テストに使った小さなデータセットでは NSD type II のほうが良い結果が出ました。</p>
<p>係数 k の値ではテストデータでの結果は変わりませんでした。</p>
<h2 id="クラスタリング">クラスタリング</h2>
<p>エンベロープ、MFCC+デルタ、ソートしたピッチは別物なのでクラスタリングを分けることにしました。以下はエンベロープ -&gt; MFCC+デルタ -&gt; ソートしたピッチの順でクラスタリングした結果の一例です。クラスタリングの階層構造がなんとなく見て取れるかと思います。 Outlier と判断されたクラスタは番号が飛んでいます。</p>
<pre><code>cluster/test2
├── envelope1
│   ├── mfccdelta2
│   │   ├── pitch1
│   │   │   ├── fast_vib2.wav
│   │   │   ├── high_vib.wav
│   │   │   ├── klang4.wav
│   │   │   ├── noisy1.wav
│   │   │   └── vib2.wav
│   │   └── pitch_outlier
│   │       └── fast_vib1.wav
│   └── mfccdelta_outlier
│       ├── ping1.wav
│       └── ping2.wav
├── envelope2
│   ├── mfccdelta0
│   │   ├── pitch1
│   │   │   ├── ding.wav
│   │   │   ├── fm.wav
│   │   │   └── noisy4.wav
│   │   └── pitch_outlier
│   │       └── sweep_to_high.wav
│   └── mfccdelta_outlier
│       └── vib1.wav
├── envelope3
│   ├── mfccdelta0
│   │   ├── pitch1
│   │   │   ├── klang1.wav
│   │   │   ├── klang3.wav
│   │   │   └── noisy3.wav
│   │   └── pitch_outlier
│   │       ├── mid_crack.wav
│   │       └── noisy5.wav
│   └── mfccdelta_outlier
│       ├── klang2.wav
│       └── vib3.wav
└── envelope_outlier
    └── noisy2.wav</code></pre>
<p>クラスタリング手法は <code>sklearn.cluster.AffinityPropagation</code> を使いました。エンベロープのクラスタリングでは <code>damping=0.9</code> 、MFCC+デルタのクラスタリングでは <code>damping=0.6</code> 、ソートしたピッチのクラスタリングでは <code>damping=0.5</code> としました。</p>
<p>次の動画はクラスタリングの結果です。音は0.25秒間隔で再生されます。残りの0.75秒は次の音と重なっています。</p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/4SleDuWeYT4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>多少は似たような音が集まっている気がします。</p>
<h2 id="その他">その他</h2>
<p>ラベルのついていないデータセットでのクラスタリングは無謀です。</p>
<p>今回使ったエンベロープとMFCC+デルタはそういう無茶でもそれなりになんとかしてくれました。エンベロープとMFCC+デルタによるクラスタリングではビブラートのような細かいピッチの揺れやカラーノイズはうまく区別できていないように感じたのでピッチ推定に注目したのですがあまり大きく改善したようには感じませんでした。</p>
<h2 id="the-infinite-drum-machine-の手法">The Infinite Drum Machine の手法</h2>
<p>Google AI Experiments の <a href="https://experiments.withgoogle.com/drum-machine">The Infinite Drum Machine</a> で使われていた手法を参考にしたので紹介します。 The Infinite Drum Machine は <a href="https://github.com/kylemcdonald/AudioNotebooks">Kyle Mcdonald さんの AudioNotebooks</a> の視覚化です。クラスタリングはインターフェイス上で音を表す点の色付けに使っているだけのようです。</p>
<ol type="1">
<li>音の始まりから250msを切り取って <a href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform">STFT</a>。</li>
<li>STFTの結果を [0, 1] の範囲に正規化。</li>
<li><a href="http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce"><code>skimage.measure.block_reduce</code></a> で正規化したSTFTの結果を縮小。</li>
<li>縮小したSTFTの結果をデータポイントとして t-SNE で2次元にマッピング。</li>
<li>マッピングした空間で K-Means を使ってクラスタリング。</li>
</ol>
<p><code>block_reduce</code> は画像の特徴抽出で使われる関数です。STFTの結果は2次元なので、画像とみなして処理できるというのは面白いです。</p>
<h1 id="参考サイト">参考サイト</h1>
<ul>
<li><a href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">Practical Cryptography</a></li>
<li><a href="https://github.com/kylemcdonald/AudioNotebooks">GitHub - kylemcdonald/AudioNotebooks: Collection of notebooks and scripts related to audio processing and machine learning.</a></li>
<li><a href="https://experiments.withgoogle.com/drum-machine">The Infinite Drum Machine by Manny Tan &amp; Kyle McDonald | Experiments with Google</a></li>
<li><a href="https://github.com/googlecreativelab/aiexperiments-drum-machine">GitHub - googlecreativelab/aiexperiments-drum-machine: Thousands of everyday sounds, organized using machine learning.</a></li>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks – the data science blog</a></li>
</ul>


  <footer>
    <a href="../index.html">インデックスに戻る</a>
  </footer>
</body>

</html>

